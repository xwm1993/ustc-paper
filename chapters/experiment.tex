% !TeX root = ../main.tex

\chapter{系统测试}
本章在DNNCL的基础上，对比采用运行时优化手段和不采用优化手段时的性能差异，验证优化技术的有效性和实用性。

\section{测试环境}
\subsection {测试方法}
本系统开发完成之后，采用黑盒与白盒测试相结合的方式，对设计的功能模块进行测试，验证系统功能的正确性。然后和优化之前的情况对比，验证系统的实用性。

\subsection{测试环境配置}
测试环境配置信息见表~\ref{tab:experiment-env}。

\begin{table}[htb]
  \centering\small
  \caption{测试配置信息说明}
  \label{tab:experiment-env}
  \begin{tabular}{ll}
    \toprule
    类别       & 配置信息   \\
    \midrule
    硬件环境   & 基于inter-x86结构的CPU \\
              & 主频≥2.4GHz       \\
              & 内存≥4G     \\
              & 硬盘空间≥10G \\
              & DianNao系列MLU100深度学习处理器 \\
    软件  & Ubuntu16.04 \\
          & DNNCL \\
          & DN\_CAFFE  \\
    网络通信  & 无 \\ 
    \bottomrule
  \end{tabular}
\end{table}

\section {性能测试}
测试所用的数据来自于ImageNet数据集，从中随机挑选了5万张带标识的图片用于测试分类网络。在进行性能测试时，每次测试5000张图片，测量10次，然后取加权平均值。

\subsection {指令缓存性能测试}
测试用例设计，指令缓存功能主要是避免重复编译，所以测试用例设计如下。，开启指令缓存功能后，用同一个网络，先后运行三次，第一次记录原始编译过程的时间；第二次开启离线模型缓存功能，记录原始编译加上保存离线模型的时间，观察新加入的过程对总流程有多大的影响；第三次，理论上能正确命中，记录命中之后总的编译时间。对比神经网络的原始编译时间、编译加保存离线模型时间，和命中后编译所需的时间，以检验命中之后是否确实避免了耗时久的编译过程；对比使用离线模型时的正确率和不用离线模型的正确率是否有差异，以检验离线缓存功能对神经网络的结果是否影响。最后用多种不同的网络进行测试，检验功能的完备性和鲁棒性，测试结果如表~\ref{tab:experiment-cache-data}所示。

\begin{table}[htb]
  \centering\small
  \caption{指令缓存测试数据}
  \label{tab:experiment-cache-data}
  \begin{tabular}{llllllll}
    \toprule
    网络      & 原始编译 & 编译加保存 &命中后编 & 第一次&正确率 &第二次&正确率 \\
              & 时间    &模型时间    &译时间   & top1 & top5  &top1 & top5   \\
    \midrule
    Resnet18  & 640.076 & 756.91  & 95.928  & 66.51 & 87.46 & 66.51 & 87.46 \\
    Resnet34  & 1090.24 & 1284.82 & 157.66  & 71.10 & 90.03 & 71.10 & 90.03  \\
    Resnet50  & 3112.29 & 3254.73 & 189.187 & 73.01 & 91.07 & 73.01 & 91.07  \\
    Resnet101 & 7075.01 & 7421.33 & 306.75  & 74.36 & 91.90 & 74.36 & 91.90  \\
    Resnet152 & 12211.2 & 12817.4 & 481.026 & 74.79 & 92.19 & 74.79 & 92.19  \\
    Vgg16     & 1562.58 & 1717.09 & 928.119 & 70.85 & 88.68 & 70.85 & 88.68  \\
    Vgg19     & 1873.26 & 2003.45 & 941.409 & 70.25 & 89.76 & 70.25 & 89.76  \\
    AlexNet   & 938.387 & 1119.93 & 423.322 & 57.13 & 80.21 & 57.13 & 80.21  \\
    GoogleNet & 1275.37 & 1334.38 & 53.8    & 68.75 & 88.37 & 68.75 & 88.37  \\
    MobileNet & 1466.71 & 1531.22 & 51.619  & 70.81 & 89.85 & 70.81 & 89.85  \\
    SequeezeNet & 747.705  & 907.378 & 298.191 & 57.07 & 80.01 & 57.07 & 80.01 \\
    Inception\_V3 & 2875.93 & 3025.54 & 165.192 & 78.23 & 94.20 & 78.23 & 94.20  \\
    \bottomrule
  \end{tabular}
  \note{注：表中编译时间单位为秒；正确率为百分数；top1表示预测的最高值直接命中，top5表示预测的前五个最高值中包含正确结果}
\end{table}

从表中时间数据可知，保存离线模型时会在原来的编译过程中会稍微增加一点时间开销，但是影响不大，影响最大的是Resnet18网络，保存离线模型的时间占到了原始时间的18.25\%，影响最小的是Resnet152网络，保存离线模型的时间子相当于原始时间的4.96\%。然而，网络模型一旦命中，确实可以极大的缩短再次编译的时间，命中后，在次编译时间缩短最大的是Resnet152网络，只有原始编译时间的3.94\%；提升最小的是Resnet18网络，但是也只有原来的14.99\%。所以模型缓存功能可以极大的缩减再次编译的时间，而且数据表明，神经网络越大，原始编译时间越长，则保存模型对原来编译时间的影响越小，模型缓存对编译时间的提升越明显。

从表中正确率数据可知，直接编译运行的正确率和加载缓存的离线模型的正确率没有任何区别。所以存储后再解析加载执行的模式对神经网络的正确率没有任何影响。

\subsection {权值量化性能测试}
权值量化的主要功能是对神经网络的权值数据进行量化处理，在存储是的时候减小存储空间，在运算的时候，利用定点数计算，提升运算速度，加速神经网络的推理过程。权值量化可以加速神经网络的推理过程，但是在一定程度上会降低神经网络的精确度。可以通过量化到不同的精度的方式，来寻求速度和精确度的平衡。由于实验阶段，硬件运算器的限制，只支持量化到Int8类型，所以测试数据只测试了量化到Int8类型时的模型大小、推理时间、已经网络模型的正确率。

量化到Int8类型网络离线模型的大小如表~\ref{tab:experiment-quant-data}所示。

\begin{table}[htb]
  \centering\small
  \caption{指令缓存测试数据}
  \label{tab:experiment-quant-data}
  \begin{tabular}{llll}
    \toprule
    网络      & 量化前模型大小 & 量化后模型大小 & 体积压缩比 \\
    \midrule
    Resnet18  & 17  & 9.6 & 1.77  \\
    Resnet34  & 35  & 19  & 1.84  \\
    Resnet50  & 35  & 20  & 1.75  \\
    Resnet101 & 73  & 37  & 1.97  \\
    Resnet152 & 181 & 63  & 2.87  \\
    Vgg16     & 187 & 134 & 1.40  \\
    Vgg19     & 269 & 211 & 1.27  \\
    AlexNet   & 44  & 39  & 1.13  \\
    GoogleNet & 16  & 6.6 & 2.42  \\
    MobileNet & 14  & 4.9 & 2.86  \\
    SqueezeNet& 3.4 & 1.8 & 1.89  \\
    Inception\_v3  & 49  & 18  & 2.72 \\
    \bottomrule
  \end{tabular}
  \note{注：表中模型大小单位是M(兆)}
\end{table}

由于编译之后保存的离线模型的大小和输入数据的规模（输入数据的规模，间接影响权值等数据的规模）有关，所以不同网络之间的大小会和网络的深度存在一定的差异，但是这并不影响我们比较定点量化网络模型的压缩的功效。从表中我们可以看出，使用定点量化之后的，神经网络模型存储空间大小都得到了较大程度的缩减。压缩比（压缩前的体积/压缩后的体积）最大的是Resnet152，压缩之后只有原来的34.8\%;效果最差的AlexNet网络也只有原来的88.64\%。所以可以说权值压缩可以极大的缩减模型的大小，节省存储空间。

\subsection {权值量化精度测试}

原始的float16网络的和量化后Int8网络的推理时间，以及正确率如表~\ref{tab:quant-result-data}所示。推理性能用单位时间能够处理图片的数量来表示，单位时间内能处理的图片数量越多，则吞吐量越高，计算越快。

\begin{table}[htb]
  \centering\small
  \caption{量化前后测试结果}
  \label{tab:quant-result-data}
  \begin{tabular}{llllllll}
    \toprule
    网络      & 推理性能& &       &正确率 & & &              \\
              &量化前 &量化后 &提升  &top1 & top5 &top1 &top5  \\ 
    \midrule
    Resnet18  & 1741.00 & 2103.00 & 20.79\%  & 66.51 & 87.46 & 66.23 & 87.60  \\
    Resnet34  & 1022.33 & 1302.67 & 27.36\%  & 71.10 & 90.03 & 70.93 & 89.95  \\
    Resnet50  & 645.67  & 781.33  & 21.01\%  & 73.01 & 91.07 & 72.91 & 91.05  \\
    Resnet101 & 369.33  & 465.33  & 25.99\%  & 74.36 & 91.90 & 74.12 & 90.37  \\
    Resnet152 & 261.33  & 326.00  & 24.75\%  & 74.79 & 92.19 & 73.98 & 91.64   \\
    Vgg16     & 289.00  & 416.67  & 44.18\%  & 70.85 & 88.68 & 70.37 & 88.53  \\
    Vgg19     & 214.00  & 335.00  & 56.54\%  & 70.25 & 89.76 & 69.79 & 89.03  \\
    AlexNet   & 996.33  & 1466.33 & 47.17\%  & 57.13 & 80.21 & 56.73 & 79.96  \\
    GoogleNet & 1307.67 & 1335.33 & 2.11\%   & 68.75 & 88.37 & 68.23 & 88.01  \\
    MobileNet & 1633.33 & 1734.33 & 6.18\%   & 70.81 & 89.85 & 69.53 & 88.76  \\
    SqueezeNet & 1842.67 & 2125.67 & 15.36\% & 57.07 & 80.01 & 56.43 & 79.21  \\
    Inception\_v3 & 855.00 & 929.67  & 8.73\% & 78.23 & 94.20 & 78.02 & 93.89  \\
    \bottomrule
  \end{tabular}
  \note{注：推理性能衡量单位image/sec：图片/每秒, 正确率单位：\%}
\end{table}

从表中数据可知，大部分网络在量化之后吞吐量都能得到较大提升，在20\%左右，效果最好的是VGG网络，吞吐量增加了50\%左右。而且正确率并没有出现大幅下降，和量化之前的差距基本在0.5\%左右。

\section {结论}
指令缓存功能命中后可以极大的缩短二次编译的时间，避免重复编译。在缓存空间设置合理的情况下，采用先进先出的原则，可以增加缓存的命中率。权值量化功能不仅能减小神经网络模型编译之后的存储体积，还能大幅度的增加推理速度，提高IO量。在神经网络正确率方面的影响也比较小，在精度要求不高的环境下，基本可以忽略。



