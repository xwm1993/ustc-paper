% !TeX root = ../main.tex

\chapter{绪论}

AlphaGo和李世石的惊天一战，让寻常百姓都知道了AI技术的强大，AI也成为“互联网”类行业的新宠儿，被认为是下一个颠覆性的技术。人工智能领域是一个数据密集的领域，传统的数据处理技术难以满足高强度、大数据的处理需求\cite{cyj}。现在的 CPU、GPU 在进行深度学习人工神经网络处理的时候，速度慢（2012年谷歌使用1.6万个通用处理器，耗时7天才训练出一个识别猫脸的深度学习神经网络），耗能多（“阿尔法狗”下一盘棋，光电费都要3000美元）。其实，早在2006年，深度学习就开始受到科研机构、工业界的高度关注。深度学习需要芯片能够提供极高的并发计算能力对大量训练数据进行处理\cite{szjxl}。传统的逻辑芯片架构，如：ARM、X86等CPU，由于其计算单元的不足，无法满足深度学习对于算力的需求。而摩尔定律随着制程工艺接近极限（10nm以下时由量子隧穿效应造成的晶体管漏电以及光刻机精度限制）而逐渐失效，意味着在传统CPU设计逻辑上通过进一步提升制程工艺和主频很难再获得更高的算力表现\cite{ljwly}。应用于深度学习领域的硬件也CPU到向“CPU+GPU”、FPGA、ASIC多方向的发展\cite{jdk}。为了在特定的硬件平台技术深度学习应用，各大厂商组织也在积极的研究和发展自身的深度学习加速库来提高硬件平台的表现。

\section{运行时优化研究意义}

运行时优化是在程序运行的过程中采集和分析程序的运行时信息，对程序中执行频率较高的代码片断进行优化，从而加速程序运行\cite{wangjin}。相比程序生命周期中其它时期的优化（编译、装载、链接、运行后），运行时优化可提供确切的程序运行事件统计和运行环境信息，从而针对当前特定的行为模式进行特定的优化\cite{frosenb}。而其它时期的优化都只针对程序的平均行为模式，当程序运行模式偏离平均值较大时优化的效果就会大打折扣，甚至导致程序的执行时间比没有优化之前还要长\cite{gzyll}。运行时优化的这种优势，使得它在近年来得到了较为广泛的研究。

运行时系统是深度学习加速库软件栈中最重要的模块之一，优化深度学习加速库的运行时系统，对提高硬件的任务处理能力和资源利用率、节省功耗以及神经网络部署都具有十分重大的意义。

\section{国内外研究现状}

基于传统CPU体系结构，针对特定模型或特定语言，运行时优化系统已经有较为广泛的研究。例如针对特定语言的c/c++程序的运行时优化研究指出运行时优化的重点是对热点路径的探测和优化以提高代码的局部性和编译指导的分支预测的正确性\cite{zxj}；针对java虚拟机中的运行时系统优化研究提出了一种具有数据流特征的Java并行程序设计模型，并针对该模型提出了一种基于运行时信息反馈的自适应优化算法,使得运行时系统可以利用数据流程序所暴露出的数据并行性，加速程序的运行\cite{fbwcy}；基于值-剖面的OpenMp运行时优化系统CCRG OpenMp指出它能够根据常见的值的组合优化并行区域，并且在运行时只有并行区代码需要重编译和管理。CCRG OpenMp基于动态重编译技术，避免了目前静态多版本技术的不足。同时，值-剖面的收集和分析由独立的动态优化器线程完成，降低了动态重编译引入的开销\cite{hcyxj}；针对多核处理器运行时优化技术的研究指出随着多核技术的发展，运行时的优化开销将被忽略，编译优化器将成为应用程序执行中的一个服务线程\cite{lxy}。

为了加速神经网络在GPU上的运算，AMD公司和NVIDIA公司都推出了自己的深度学习加速库MIOpen\cite{miopen}和cuDNN\cite{cudnn}。MIOpen 实现了深度卷积解算器在正向和反向的优化，包括Winograd和快速傅立叶转换的卷积优化；实现了面向深度学习的广义矩阵乘算法(GEMM)；Pooling、SoftMax、Active、梯度算法的批量归一化，以及LR 归一化等加速优化手段。cuDNN十分注重内存的开销，强调易用性和性能，利用将卷积计算转变成在GPU上更为友好的矩阵运算的手段，来提高卷积计算的性能，并利用高性能的并行计算来加速整个深度学习的过程。不过cuDNN和MIOpen都有一个共同的不足就是，算法支持速度较慢，只提供对非常成熟的网络算法(Convolution, Pooling, SoftMax, Active(Relu, Sigmoid, TANH))的支持，对于学术上最新提出神经网络算法，例如优化后的循环神经网络，不能及时的支持\cite{yzg}。

TensorRT\cite{tensorrt}是一款由NVIDIA推出的基于CUDA\cite{cuda}和cuDNN编程模型的推理引擎，不支持训练，只支持神经网络的推理。他的主要目标是在便于在实际的生产环境中部署深度学习应用程序，比如现在比较成熟的目标检测，图像分割，图像分类等。TensorRT之所以能提升神经网络的推理速度，主要在于其使用了两种优化手段。首先TensorRT支持神经网络的量化，支持将浮点型的神经网络转换成量化后的定点型神经网络，然后使用定点数据进行计算，减少计算量。量化会能提升计算速度，但是同时也会影响神经网络的精度，TensorRT在量化和保持精度之间达到一个理想的平衡，达到加速推断的目的。其次是TensorRT对于用户的原始网络结构进行了优化和重构\cite{tensorrtinference}，优化后重构的手段主要体现在以下几个方面：1）解析网络后，消除神经网络中无用的输出层以减小计算量；2）网络结构垂直融合，简单来说将是将几个可以可以合并到一起的层融合到一个层中做，常见的是将conv、BN、Relu三个层融合为了一个层；3）网络结构水平融合，水平融合是指将输入相同，即输入是同一个张量，且执行的操作也相同的层融合一起。和其它的深度学习框架的推理速度相比，在相同的硬件条件下，TensorRT能提供10倍甚至100倍的加速，极大的提升了深度学习模型在边缘设备上的推断速度。

之后，为了解决深度学习在视频领域实时性需求，NVIDIA继续推出其新一代针对视频分析处理的高性能加速库DeepStream\cite{deepstream}。DeepStream SDK能帮助开发人员快速的构建高效、高性能的视频分析应用程序。视频分析的核心依然是图像分类、目标检测、识别和跟踪等标准的计算机视觉任务。高性能的应用程序建立在流水线上，通过将视频拆解成神经网络所需要的分辨率和格式的视频帧，从而最大化吞吐量。可扩展性要求并行处理多个视频流以获得更高的信道密度（在给定空间中处理的视频信道数量）\cite{zhangwei}。DeepStream SDK提供包含对输入视频流解码、预处理和推理的模块，所有模块都经过精细调整，以提供最大的帧吞吐量\cite{lzl}。这些模块紧密联系，以确保在正确使用数据传输和软件同步的同时获得最大的硬件并发性。不足的是，目前DeepStream只支持基于Caffe的网络\cite{lgf}。

为了解决CPU+GPU在算力、功耗和任务分配上的限制，寒武纪团队2014发表的DianNao论文提出首个多核的深度学习专用处理器架构，与主流 GPU相比，取得了21倍的性能和300倍的性能功耗比\cite{chent}；2015发表的PuDianNao提出首个通用的机器学习处理器，实现了包括k-最近邻、 k-均值、朴素贝叶斯、线性回归等7种机器学习算法的兼容，平均性能与主流GPGPU相当，但面积和功耗仅为主流GPGPU百分之一\cite{liuD}；2016年提出全球首个神经网络通用指令集架构，兼容十种代表性的神经网络，针对大规模的神经元计算，单条指令即可完成一次向量或矩阵运算\cite{lius}。随后也推出了基于自身硬件平台的机器学习加速库DNNCL,极大的提升了神经网络的推理速度\cite{cheny}。

近年来，随着深度学习算法、人工智能芯片以及深度学习加速库的快速发展，面向神经网络计算库的运行时优化技术也变得更加迫切。

\section{本文主要工作}

深度学习加速库的主要作用包括构建计算图，动态编译生成指令，拷入输入数据，任务调度、计算结果等过程，其中动态编译生成指令、拷贝输入输出数据、任务调度、计算等过程都是运行时过程。本文针对深度学习先构建计算图后计算，以及神经网络结构大多相同的特性，提出一些可以优化神经网络的运行时系统的方法，并在寒武纪深度学习加速库DNNCL的基础上实现了这些方法。实验结果表明，当采用指令缓存技术时，能避免相同神经网路的重复编译，在二次编译的情况下该方法能大幅度节省程序编译时间，提高整体的运行效率；采用权值量化技术的时候，能大幅减少神经网络模型文件的存储空间，减少神经网络部署时的内存占用。

本论文的主要内容包括：
\begin{enumerate}
  \item 调研当前主流的深度学习加速库，研究运行时优化策略。并深入理解DNNCL的编程模型、运行原理，掌握基于DNNCL构建神经网络图、编译指令、部署网络的整个流程。
  \item 实现用户计算图的快速识别并保存。计算图包含了整个神经网络具体的计算任务和对应的数据描述信息，以及计算任务的前后依赖关系。只有在识别了用户的计算意图之后，才能够进行相应的优化处理。
  \item 实现面向神经网络模型的指令存储技术。该技术能够将用户基于深度学习加速库搭建的应用程序编译之后生成的指令保存到文件中，运算时支持直接从文件中解析出指令和数据然后执行相应的计算任务。
  \item 实现面向神经网络的指令缓存技术。在1、2的基础上，如果识别出用户要执行的计算图在之前已经编译运行过，则省去当前的编译过程，直接加载之前缓存的指令，集合这次的输入数据，进行后续的计算过程。
  \item 实现了一种神经网络模型的压缩技术，对权值数据进行量化处理，在精度损失可接受的范围内，能大幅减少了神经网络模型的存储空间。
\item 在DNNCL的环境下，对这些优化方法进行编码实现，测试分析，得出结论。
\end{enumerate}

\section {本文组织结构}
本文主要描述在运行时过程中优化技术的背景，设计思路和编码实现，本文的组织结构和安排如下。

第一章：绪论。本章主要介绍和分析面向深度学习加速库中运行时优化技术的研究背景和意义，对运行时优化技术的国内外研究现状进行归纳总结，并对本文的关键内容和组织结构进行介绍说明。

第二章：深度学习加速库简介。本章主要介绍和分析现阶段主流的神经网络计算库之间的异同点，并对本文的实验环境DNNCL做了详细介绍，主要介绍其编程模型和运行流程。再此基础上，实现并验证本文的优化技术。

第三章：需求分析。本章主要从运行时优化的角度出发，结合用户的实际使用场景，分析运行时优化的功能和性能需求。

第四章：概要设计。本章主要描述软件的整体结构，进而对软件的主要模块进行独立设计。如：计算图保存模块、指令存储模块、指令缓存模块、指令加载模块、调度优化模块。

第五章：详细设计与实现。本章节主要是对神经网络框架总体架构方面的信息、系统的相关类图信息、功能模块设计等进行详细的阐述与分析，同时展示实现的效果图。

第六章：系统测试。主要是从软件测试的角度对系统的正确性、可靠性及主要功能模块进行测试分析，包括系统的测试环境、性能测试、测试结果分析等。

第七章：总结和展望。本章对该篇论文进行简单的总结，同时对运行时优化存在的不足进行分析，结合未来的发展对面向神经网络加速库运行时优化技术进行了展望。

\section {本章小结}

本章首先介绍了本论文的研究背景和意义，以及国内外的研究现状。关于常规应用程序或者特定语言的运行时优化技术有较多的研究，但是针对深度学习加速库运行时优化技术的研究还比较少。然后简要介绍本文的主要工作，提出并实现了一些针对深度学习加速库运行时优化的技术。最后简要介绍了本论文的组织安排，方便读者查阅。