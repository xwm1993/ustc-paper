% !TeX root = ../main.tex

\chapter{总结和展望}

\section {总结}

本文对比研究了多个深度学习加速库的编程模型和运行流程，分析深度学习加速库的整体架构，从而提出优化运行时这一关键瓶颈的想法。之后在DNNCL的基础上，立足于运行时优化的需求分析，详细介绍了整个优化系统和各个模块之间的概要设计和详细设计，给出了主要模块和功能的处理流程和类图以及各个模块详细的设计思路。本文不仅仅是笔者对自身工作的总结，也为后续的研究和开放人员提供了方向和宝贵的参考资料。

本文的主要工作有以下几个方面：
\begin{enumerate}
  \item 本文说明了面向深度学习加速库运行时优化技术的重要意义和作用，详细概括了现代深度学习库的“细腰”结构和设计理念。基于这种结构和设计理念，为提高深度学习库的表现提供了一些优化方向和思路。
  \item 面向优化深度学习加速库的运行时阶段进行需求分析，挖掘系统需求，确定优化系统需要实现什么，具有什么样的功能。
  \item 设计和实现深度学习加速库的运行时优化系统，完成各个模块的开发，支持优化的功能。主要特点在于：本文提出了逆向还原用户计算图结构的想法，并给出了利用JSON文件保存计算图结构的实现方法，可拓展性良好，支持各种上层框架，适用于各种硬件环境；文中设计了一种离线保存指令模型的文件结构并加以实现，支持模型离线保存和动态加载，为神经网络模型在嵌入式设备上的部署提供了一种新的方向；基于计算图识别和指令的离线保存，提出了指令缓存的想法，避免使用相同神经网络结构的应用程序的重复编译，大大缩短了应用程序的编译时间，提高了运行效率；将神经网络的结构和权值信息分开保存，结合权值更新技术，提高了缓存的命中率；为了减少神经网络模型的存储空间，提出利用权值量化来压缩神经网络模型的方法，保证精度损失在可接受范围内可大大减少模型的存储空间。
\end{enumerate}

总而言之，本课题设计和实现基于深度学习加速库运行时的优化系统，采用指令缓存、权值更新等手段来尽量减少神经网络的编译时间，优化编译过程；采用权值量化技术来减少模型的体积，提高存储空间的利用率。

\section{展望}
目前本系统对编译阶段的优化是从宏观上避免相同网络的重复编译，但是对于新出现的网络结构，还是需要经过完整的编译过程，时间较长，对编译的内部流程没做优化处理。目前系统中整张图的编译，是按照调度之后给出的拓扑序进行编译的。其实不连通的子图之间的编译是互不依赖的，可以将整张图按照连通性进行拆解，分成多张图利用多线程进行分别编译，将会减少编译时间。

在构建阶段没做优化处理，实际上在构建阶段的内部可以做图优化，除了像TensorRT那样将固定的垂直结构融合成一个层，在多核的情况下，应该还可以将计算图水平拆分，例如将一个大的conv拆分成几个小的conv，每个小的conv处理其中的一部分，在拼接处的数据用特殊的算子进行处理。这样就可以将一个大的算子拆成几个可以并行执行的小的算子，提高计算任务的并行度，从而加快网络的推理过程。

在神经网络加速库的内存管理中，可以增加内存复用模块，在推理过程中，某些使用完的数据是可以释放，不需要一直占用内存空间的，在为后面的数据分配地址的时候，可以考虑复用之前分配过的地址空间。如果不采用内存复用，所需要的内存空间可能是所有数据量的总和，使用内存复用，需要的空间理想情况下是共享数据的最大值。
